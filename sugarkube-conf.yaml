log-level: none
#json-logs: false
#no-color: true

# Dynamically searches for terraform tfvars files based on the current stack provider and various properties of the
# stack (e.g. name, region, etc.) as well as any generated files. All files found are prepended by `-var-file`
tf-patterns: &tf-patterns
  tf-params: >-
    {{ mapPrintF "terraform_%s/.*defaults\\.tfvars$" (listString .stack.provider) | findFiles .kapp.cacheRoot | mapPrintF "-var-file %s" | uniq | join " " | trim }}
    {{ mapPrintF (mapPrintF "terraform_%s/.*%%s\\.tfvars$" (listString .stack.provider) | join "") (.sugarkube.defaultVars | removeEmpty) | findFiles .kapp.cacheRoot | mapPrintF "-var-file %s" | uniq | join " " | trim }}
    {{ mapPrintF "terraform_%s/.*_generated_.*\\.tfvars$" (listString .stack.provider) | findFiles .kapp.cacheRoot | mapPrintF "-var-file %s" | uniq | join " " | trim }}

# Search for parameters dynamically. The `findFiles` function takes a list of
# file name patterns and a starting directory to search from. It then recursively
# searches for a single file matching the pattern. If one (and only one) is found,
# it's returned in the result array. Patterns that don't match any files are
# filtered out.
#
# The `.sugarkube.defaultVars` variable is a list populated at runtime containing (in
# order of precedence: provider, account, profile, cluster, region.
#
# So these scary looking things search for a values.yaml file in the kapp cache directory, as well as
# `values-<provider/account/profile/etc>.yaml` and prepends '-f' ready to be passed as options to helm.
helm-patterns: &helm-patterns
  helm-params: >-
    {{ listString "/values\\.yaml$" | findFiles .kapp.cacheRoot | mapPrintF "-f %s" | uniq | last | join " " | trim }}
    {{ mapPrintF "/values-%s\\.yaml$" (.sugarkube.defaultVars | removeEmpty) | findFiles .kapp.cacheRoot | mapPrintF "-f %s" | uniq | join " " | trim }}
    {{ listString "/_generated_.*\\.yaml$" | findFiles .kapp.cacheRoot | mapPrintF "-f %s" | uniq | join " " | trim }}

programs:
  helm:
    vars:
      kubeconfig: "{{ .kubeconfig }}"
      namespace: "{{ .kapp.id }}"
      release: "{{ .kapp.id }}"
      kube_context: "{{ .kube_context }}"

    # todo - uncomment once viper supports not lowercasing all map keys
    #    env_vars:
    #      RUN_HELM: |-
    #        "{{ if isSet .kapp.vars "run_helm" }}{{ .kapp.vars.run_helm }}{{ else }}true{{ end }}"

    args:
      make:
        install:
          <<: *helm-patterns
        delete:
          <<: *helm-patterns

    run_units:
      vars:
        helm: helm
        run_helm: true    # none of these units will be run for a kapp if this is false because of the associated condition
      working_dir: "{{ .kapp.cacheRoot }}"       # directory to use as the working directory
      conditions:       # all must be true for any units to be run
        - {{ .kapp.vars.run_helm }}
      plan_install:
        - command: |
            KUBECONFIG={{ .kapp.vars.kubeconfig }} {{ .kapp.vars.helm }} lint --kube-context={{ .kapp.vars.kube_context }} \
              . --namespace={{ .kapp.vars.namespace }}
      apply_install:
        - command: |
            KUBECONFIG={{ .kapp.vars.kubeconfig }} {{ .kapp.vars.helm }} upgrade --kube-context={{ .kapp.vars.kube_context }} \
              --tiller-namespace={{ .kapp.vars.tiller_namespace }} --wait --install --recreate-pods --timeout 600 \
              {{ .kapp.vars.release }} . \
              --namespace={{ .kapp.vars.namespace }}
      apply_delete:
        - command: |
            KUBECONFIG={{ .kapp.vars.kubeconfig }} {{ .kapp.vars.helm }} delete --kube-context={{ .kapp.vars.kube_context }} \
              --tiller-namespace={{ .kapp.vars.tiller_namespace }} --purge {{ .kapp.vars.release }}

  kubectl:
    vars:
      kubeconfig: "{{ .kubeconfig }}"
      kube_context: "{{ .kube_context }}"
      namespace: "{{ .kapp.id }}"

  aws:
    vars:
      region: "{{ .stack.region }}"

  terraform:
    vars:
      region: "{{ .stack.region }}"
      project: "{{ .project }}"

    args:
      make:
        install:
          <<: *tf-patterns
        delete:
          <<: *tf-patterns
        output:
          <<: *tf-patterns

    run_units:
      vars:
        run_terraform: true    # none of these units will be run for a kapp if this is false because of the associated condition
        terraform: terraform
        terraform_dir: terraform_{{ .stack.provider }}
      working_dir: "{{ .kapp.cacheRoot }}/{{ .kapp.vars.terraform_dir }}"       # directory to use as the working directory
      conditions:       # all must be true for any units to be run
        - {{ .kapp.vars.run_terraform }}
      plan_install:
        - name: tf-init
          command: "{{ .kapp.vars.terraform }} init"
          conditions:       # additional conditions for this specific step to be run
          - ! -d "{{ .kapp.cacheRoot }}/{{ .kapp.vars.terraform_dir }}/.terraform"
        - name: tf-format
          command: "{{ .kapp.vars.terraform }} fmt"
        - name: tf-validate
          command: "{{ .kapp.vars.terraform }} validate"
        - name: tf-plan
          command: "{{ .kapp.vars.terraform }} plan -refresh=true -out _generated_plan.tfplan"
      apply_install:
        - name: tf-apply
          command: "{{ .kapp.vars.terraform }} apply _generated_plan.tfplan"
      apply_delete:
        - name: tf-init
          command: "{{ .kapp.vars.terraform }} init"
          conditions:       # additional conditions for this specific step to be run
          - ! -d "{{ .kapp.cacheRoot }}/{{ .kapp.vars.terraform_dir }}/.terraform"
        - name: tf-destroy
          command: "{{ .kapp.vars.terraform }} destroy -auto-approve"

# globally defined run units will be used as defaults for each program
run_units:
  clean:
    - command: "rm {{ .kapp.cacheRoot }}/*.log"
    - command: "rm {{ .kapp.cacheRoot }}/*.err"
    - command: "find {{ .kapp.cacheRoot }} -name '_generated_*' -type f -delete"
    - command: "find {{ .kapp.cacheRoot }} -name '_generated_*' -type d -exec rm -r {} ';'"
